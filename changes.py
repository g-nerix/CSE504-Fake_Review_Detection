# -*- coding: utf-8 -*-
"""IR_Final_Code (5).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ICbWgDWUEoWNR75KO7EK93DBhFgErcO
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""## Fake Review Detection System"""


"""## Importing Packages"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import random
import os
import gc
from PIL import Image
from textwrap import wrap
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy
from tensorflow.python.keras import models, layers, optimizers
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.convolutional import Conv1D
from keras.layers import Embedding
from keras.layers import GlobalMaxPooling1D
from keras.layers.core import Activation, Dropout, Dense
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
# from transformers import DistilBertConfig, TFDistilBertForSequenceClassification
import bz2
import re

# %matplotlib inline

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



# Any results you write to the current directory are saved as output.

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tokenizers import BertWordPieceTokenizer

import transformers
from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

import logging
import streamlit as st

import random
import os
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
transformers.logging.set_verbosity_error()


import transformers
from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification

import pandas as pd
import numpy as np
import random
import os
import gc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
import pandas as pd
import numpy as np
import random
import os
import gc
from PIL import Image
from textwrap import wrap
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import bz2
import re
import tensorflow as tf
from tensorflow.python.keras import models, layers, optimizers
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.convolutional import Conv1D
from keras.layers import Embedding
from keras.layers import GlobalMaxPooling1D
from keras.layers.core import Activation, Dropout, Dense
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from transformers import DistilBertConfig, TFDistilBertForSequenceClassification

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tokenizers import BertWordPieceTokenizer

import transformers
from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

import logging
transformers.logging.set_verbosity_error()

train = "train.ft.txt.bz2"
# test = "/content/drive/MyDrive/test.ft.txt.bz2"
users = "Reviews.csv"

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# !pip install transformers

import transformers
from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification

tf.random.set_seed(18)
np.random.seed(18)
import random

df = pd.read_csv('file.csv')
data_dict = {}
options = []
# iterate through each row in the dataframe
for index, row in df.iterrows():
    # get the product ID and text values
    product_id = row['ProductId']
    text = row['Text']
    options.append(product_id)
    # if the product ID is not already a key in the dictionary, add it and initialize the value with an empty array
    if product_id not in data_dict:
        data_dict[product_id] = []
        
    # append the text value to the array for the product ID key
    data_dict[product_id].append(text)



from sklearn.model_selection import train_test_split
train_labels, test_labels, train_texts , test_texts = train_test_split(df['Label'],df['Text'] ,
                                   random_state=104, 
                                   test_size=0.25, 
                                   shuffle=True)

NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):
  
    normalized_texts = []
    for text in texts:
        lower = text.lower()
        no_punctuation = NON_ALPHANUM.sub(r' ', lower)
        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
        normalized_texts.append(no_non_ascii)
    return normalized_texts
        
train_texts = normalize_texts(train_texts)
test_texts = normalize_texts(test_texts)

train_texts_Bert = train_texts
train_labels_Bert = train_labels
test_texts_Bert = test_texts
test_labels_Bert = test_labels
train_reviews = train_texts

"""#### Train/Validation Split
We split the dataset, keeping 20% of the training set for validation.
"""

train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, random_state=57643892, test_size=0.2)

MAX_FEATURES = 12000
#Tokenize texts
tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(train_texts)
train_texts = tokenizer.texts_to_sequences(train_texts)
val_texts = tokenizer.texts_to_sequences(val_texts)
test_texts = tokenizer.texts_to_sequences(test_texts)

# print(train_texts[0])

MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)
#Add padding
train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)
val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)
test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)

# st.write(train_texts[0])

"""### Long Short-Term Memory (LSTM)"""

def build_lstm_model():
    sequences = layers.Input(shape=(MAX_LENGTH,))
    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)
    x = layers.LSTM(32, return_sequences=True)(embedded)
    x = layers.GlobalMaxPool1D()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dropout(0.2)(x)
    predictions = layers.Dense(1, activation='sigmoid')(x)
    model = models.Model(inputs=sequences, outputs=predictions)
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model
    
lstm_model = build_lstm_model()

print(lstm_model.summary())

# lstm_model.fit(
#     train_texts, 
#     train_labels, 
#     batch_size=128,
#     epochs=2,
#     validation_data=(val_texts, val_labels), )

preds = lstm_model.predict(test_texts)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))
print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))
print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))

st.write('Accuracy score: 0.8735')
st.write('F1 score: 0.8909')
st.write('ROC AUC score: 0.9423')
# lstm_model.save_weights('lstm.h5')

model1 = build_lstm_model()
model1.load_weights('lstm.h5')

# input_text = "Good product, must buy !!"
input_text = st.text_input("Enter query here:" , key='first_input')
print('printing', input_text)
if st.button('Submit',key = 'first'):
    print('clicked')
    st.write('Processed text:', input_text)
    input_text = tokenizer.texts_to_sequences([input_text])
    input_text = pad_sequences(input_text, maxlen=MAX_LENGTH)
    #prediction = lstm_model.predict(input_text)
    prediction = model1.predict(input_text)
    label = 1 if prediction > 0.5 else 0
    print(label)
    st.write(label)

selected_option = st.selectbox('Select an option', options)
for val in data_dict[selected_option]:
    st.write(val)
    print(val)




#User REcomendation

top_voted_users={}
least_voted_users={}
for i in range(len(df)):
  if df['HelpfulnessNumerator'].iloc[i] / df['HelpfulnessDenominator'].iloc[i] >= 0.8 :
    if(df['UserId'].iloc[i] in top_voted_users.keys()): 
      top_voted_users[df['UserId'].iloc[i]] += 1 
    else: 
      top_voted_users[df['UserId'].iloc[i]] = 1 
  elif df['HelpfulnessNumerator'].iloc[i] / df['HelpfulnessDenominator'].iloc[i] <=0.4 :
    if(df['UserId'].iloc[i] in least_voted_users.keys()): 
      least_voted_users[df['UserId'].iloc[i]] += 1 
    else: 
      least_voted_users[df['UserId'].iloc[i]] = 1

## Sorting top recommended users dictionary
keys = list(top_voted_users.keys())
values = list(top_voted_users.values())
sorted_value_index = np.argsort(values)[::-1]
top_voted_users = {keys[i]: values[i] for i in sorted_value_index}

## Sorting least recommended users dictionary

keys = list(least_voted_users.keys())
values = list(least_voted_users.values())
sorted_value_index = np.argsort(values)[::-1]
least_voted_users = {keys[i]: values[i] for i in sorted_value_index}

### print top 10 most recommended users whose helpfulness score >3
import itertools
top_users = dict(itertools.islice(top_voted_users.items(), 10))
print("    User        :  No of recommendations")
# st.write(top_users)

### print top 10 least recommended users whose helpfulness score <3
least_rec_users = dict(itertools.islice(least_voted_users.items(), 10))
print("    User        :  No of recommendations")
# st.write(least_rec_users)

"""# Product Recommendation"""

print('Number of unique USERS in Raw data = ', df['UserId'].nunique())
print('Number of unique ITEMS in Raw data = ', df['ProductId'].nunique())
st.write('Number of unique USERS in Raw data = ', df['UserId'].nunique())
st.write('Number of unique ITEMS in Raw data = ', df['ProductId'].nunique())

most_rated = df.groupby('UserId').size().sort_values(ascending=False)[:10]
st.write(most_rated)

counts = df['UserId'].value_counts()
df_final = df[df['UserId'].isin(counts[counts >= 5].index)]

# print('Number of users who have rated 5 or more items =', len(df_final))
# print('Number of unique USERS in final data = ', df_final['UserId'].nunique())
# print('Number of unique ITEMS in final data = ', df_final['ProductId'].nunique())


# st.write('Number of users who have rated 5 or more items =', len(df_final))
# st.write('Number of unique USERS in final data = ', df_final['UserId'].nunique())
# st.write('Number of unique ITEMS in final data = ', df_final['ProductId'].nunique())

final_ratings_matrix = pd.pivot_table(df_final,index=['UserId'], columns = 'ProductId', values = "Score")
final_ratings_matrix.fillna(0,inplace=True)
print('Shape of final_ratings_matrix: ', final_ratings_matrix.shape)
given_num_of_ratings = np.count_nonzero(final_ratings_matrix)
print('given_num_of_ratings = ', given_num_of_ratings)
possible_num_of_ratings = final_ratings_matrix.shape[0] * final_ratings_matrix.shape[1]
print('possible_num_of_ratings = ', possible_num_of_ratings)
density = (given_num_of_ratings/possible_num_of_ratings)
density *= 100
print ('density: {:4.2f}%'.format(density))

final_ratings_matrix.tail()

final_ratings_matrix_T = final_ratings_matrix.transpose()
final_ratings_matrix_T.head()

train_data, test_data = train_test_split(df_final, test_size = 0.3, random_state=0)

train_data_grouped = train_data.groupby('ProductId').agg({'UserId': 'count'}).reset_index()
train_data_grouped.rename(columns = {'UserId': 'score'},inplace=True)
train_data_grouped.head()

#Sort the products on recommendation score 
train_data_sort = train_data_grouped.sort_values(['score', 'ProductId'], ascending = [0,1]) 
      
#Generate a recommendation rank based upon score 
train_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first') 
          
#Get the top 5 recommendations 
popularity_recommendations = train_data_sort.head(5) 
# st.write(popularity_recommendations)

# Use popularity based recommender model to make predictions
def recommend(user_id):     
    user_recommendations = popularity_recommendations 
          
    #Add user_id column for which the recommendations are being generated 
    user_recommendations['UserId'] = user_id 
      
    #Bring user_id column to the front 
    cols = user_recommendations.columns.tolist() 
    cols = cols[-1:] + cols[:-1] 
    user_recommendations = user_recommendations[cols] 
          
    return user_recommendations

find_recom = [15,121,200]   # This list is user choice.
for i in find_recom:
    print("Here is the recommendation for the userId: %d\n" %(i))
    print(recommend(i))    
    print("\n")

df_CF = pd.concat([train_data, test_data]).reset_index()
df_CF.tail()

#User-based Collaborative Filtering
# Matrix with row per 'user' and column per 'item' 
pivot_df = pd.pivot_table(df_CF,index=['UserId'], columns = 'ProductId', values = "Score")
pivot_df.fillna(0,inplace=True)
print(pivot_df.shape)
pivot_df.head()

pivot_df['user_index'] = np.arange(0, pivot_df.shape[0], 1)
pivot_df.head()

pivot_df.set_index(['user_index'], inplace=True)

# Actual ratings given by users
pivot_df.head()

# Convert dense matrix to CSR sparse matrix
pivot_csr = csr_matrix(pivot_df)

# Singular Value Decomposition
U, sigma, Vt = svds(pivot_csr, k=50)

# Construct diagonal array in SVD
sigma = np.diag(np.float32(sigma))

all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) 

# Predicted ratings
preds_df = pd.DataFrame(all_user_predicted_ratings, columns = pivot_df.columns)
preds_df.head()

def recommend_items(userID, pivot_df, preds_df, num_recommendations):
      
    user_idx = userID-1 # index starts at 0
    
    # Get and sort the user's ratings
    sorted_user_ratings = pivot_df.iloc[user_idx].sort_values(ascending=False)
    #sorted_user_ratings
    sorted_user_predictions = preds_df.iloc[user_idx].sort_values(ascending=False)
    #sorted_user_predictions

    temp = pd.concat([sorted_user_ratings, sorted_user_predictions], axis=1)
    temp.index.name = 'Recommended Items'
    temp.columns = ['user_ratings', 'user_predictions']
    
    temp = temp.loc[temp.user_ratings == 0]   
    temp = temp.sort_values('user_predictions', ascending=False)
    print('\nBelow are the recommended items for user(user_id = {}):\n'.format(userID))
    print(temp.head(num_recommendations))
    st.write(temp.head(num_recommendations))
    
    

#Enter 'userID' and 'num_recommendations' for the user #


rmse_df = pd.concat([final_ratings_matrix.mean(), preds_df.mean()], axis=1)
rmse_df.columns = ['Avg_actual_ratings', 'Avg_predicted_ratings']
print(rmse_df.shape)
rmse_df['item_index'] = np.arange(0, rmse_df.shape[0], 1)
rmse_df.head()

RMSE = round((((rmse_df.Avg_actual_ratings - rmse_df.Avg_predicted_ratings) ** 2).mean() ** 0.5), 5)
print('\nRMSE SVD Model = {} \n'.format(RMSE))
st.write('\nRMSE SVD Model = {} \n'.format(RMSE))

# Enter 'userID' and 'num_recommendations' for the user #

    
    
userID = st.number_input("Enter your UserID", min_value=1, max_value=50000, step=1, value=200)
num_recommendations = 5
recommend_items(userID, pivot_df, preds_df, num_recommendations)



"""### BERT"""

from sklearn.model_selection import train_test_split
train_labels, test_labels, train_texts , test_texts = train_test_split(df['Label'][:36000],df['Text'][:36000] ,
                                   random_state=104, 
                                   test_size=0.25, 
                                   shuffle=True)

NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):
    normalized_texts = []
    for text in texts:
        lower = text.lower()
        no_punctuation = NON_ALPHANUM.sub(r' ', lower)
        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
        normalized_texts.append(no_non_ascii)
    return normalized_texts
        
train_texts = normalize_texts(train_texts)
test_texts = normalize_texts(test_texts)

train_texts_Bert = train_texts
train_labels_Bert = train_labels
test_texts_Bert = test_texts
test_labels_Bert = test_labels
train_reviews = train_texts

# train_texts_Bert

train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, random_state=57643892, test_size=0.2)

MAX_FEATURES = 12000
#Tokenize texts
tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(train_texts)
train_texts = tokenizer.texts_to_sequences(train_texts)
val_texts = tokenizer.texts_to_sequences(val_texts)
test_texts = tokenizer.texts_to_sequences(test_texts)

MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)
#Add padding
train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)
val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)
test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)

train_texts_Bert, val_texts_Bert, train_labels_Bert, val_labels_Bert = train_test_split(
    train_texts_Bert, train_labels_Bert, random_state=57643892, test_size=0.2)

#Assign tokenizer object to the tokenizer class
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# train_texts_Bert

# train_encodings = tokenizer(train_texts_Bert,
#                             truncation=True,
#                             padding=True,
#                             max_length=256)
# val_encodings = tokenizer(val_texts_Bert,
#                           truncation=True,
#                           padding=True,
#                           max_length=256)
# train_dataset = tf.data.Dataset.from_tensor_slices((
#                             dict(train_encodings),
#                             train_labels_Bert
#                             ))
# val_dataset = tf.data.Dataset.from_tensor_slices((
#                             dict(val_encodings),
#                             val_labels_Bert
#                             ))
# test_encodings = tokenizer(test_texts_Bert,
#                           truncation=True,
#                           padding=True,
#                           max_length=256)
# test_dataset = tf.data.Dataset.from_tensor_slices((
#                             dict(test_encodings),
#                             test_labels_Bert
#                             ))

# bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)

# print(bert_model.summary())

# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)
# bert_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=["binary_accuracy"])
# bert_model.fit(train_dataset.shuffle(100).batch(16), 
#           epochs=2,
#           batch_size=16,
#           validation_data=val_dataset.shuffle(100).batch(16))

# results_bert = bert_model.evaluate(test_dataset.batch(8))

# bert_model.save_weights('/content/drive/MyDrive/irfinalproject/bert.h5')

bert_model1 = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)

bert_model1.load_weights('bert.h5')
st.write('Accuracy score: 0.8918')
st.write('F1 score: 0.912')
st.write('ROC AUC score: 0.962')
# query = ""
query = st.text_input("Enter query here:" , key='bert_input')
print('printing', query)
if st.button('Submit',key='second'):
    print('clicked')
    st.write('Processed Query: ' , query)
    encoding = tokenizer.encode_plus(query, max_length=MAX_LENGTH, truncation=True, padding='max_length', return_tensors='tf')
    outputs = bert_model1(encoding)
    #rel_val = outputs.logits.numpy()[0]
    predicted_label = outputs.logits.numpy()[0] > 0.5  # assuming threshold of 0.5
    print(predicted_label[0])
    st.write(predicted_label[0])