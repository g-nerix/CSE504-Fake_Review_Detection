# -*- coding: utf-8 -*-
"""frontend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1krGGRVBslJPSfmI5dEK9m_cQTGIG9U5K
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC 
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve,auc,roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, RocCurveDisplay
from tensorflow.python.keras.utils.np_utils import to_categorical
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing import sequence
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score    
from tensorflow.python.keras import models, layers, optimizers   
from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, SpatialDropout1D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
import tensorflow as tf
import pickle
import warnings
import matplotlib.pyplot as plt
warnings.simplefilter("ignore")

# from google.colab import drive
# drive.mount('/content/drive')

df = pd.read_csv("cleaned_data.csv",encoding="latin1")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
df.drop(['Unnamed: 0'], axis=1, inplace=True)

df = df.dropna(how='any',axis=0)

df["verified_purchase"].value_counts()

X = df['review_text'] #input var
y = df['verified_purchase'] #target var

X_train, X_test, y_train, y_test = train_test_split(
    df['review_text'], df['verified_purchase'],test_size=0.4, random_state=123)

entiredf = format(df.shape[0])
traindf = format(X_train.shape[0])
testdf = format(X_test.shape[0])

print('Number of rows:')
print('Entire dataset:', entiredf)
print('Train dataset:', traindf)
print('Test dataset:',testdf)

count_vectorizer  = CountVectorizer(stop_words='english')
count_vectorizer.fit(X_train)
print('Vocabulary:', count_vectorizer.vocabulary_)

train_c = count_vectorizer.fit_transform(X_train)
test_c = count_vectorizer.transform(X_test)

"""### Multinomial Naive Bayes model"""

#IMPLEMENTING AND RUNNNING MNB MODEL - COUNT
mnb1 = MultinomialNB()
mnb1.fit(train_c, y_train)
prediction = mnb1.predict(test_c)

#EVALUATION
mnb_a1 = accuracy_score(y_test, prediction)*100
mnb_p1 = precision_score(y_test, prediction)* 100
mnb_r1 = recall_score(y_test, prediction)*100
mnb_f11 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
import streamlit as st
cm =  confusion_matrix(y_test, prediction, labels=mnb1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=mnb1.classes_) 
display.plot() 
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

# Commented out IPython magic to ensure Python compatibility.

# import streamlit as st

options = ["Option 1", "Option 2", "Option 3"]
selected_option = st.selectbox("Select an option", options)

st.write("You selected:", selected_option)


fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.show()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")
fig.show()

# Display the plot in Streamlit
st.pyplot(fig)

from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(mnb1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### Support Vector Machine model"""

#IMPLEMENTING AND RUNNNING SVM MODEL - COUNT
svm1 = SVC(kernel='linear')
svm1.fit(train_c, y_train)
prediction = svm1.predict(test_c)

#EVALUATION
svm_a1 = accuracy_score(y_test, prediction)*100
svm_p1 = precision_score(y_test, prediction)* 100
svm_r1 = recall_score(y_test, prediction)*100
svm_f11 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=svm1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=svm1.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")

# Display the plot in Streamlit
st.pyplot(fig)

from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(svm1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### Logistic Regression model"""

#IMPLEMENTING AND RUNNNING LR MODEL - COUNT
lr1 = LogisticRegression()
lr1.fit(train_c, y_train)
prediction = lr1.predict(test_c)

#EVALUATION
lr_a1 = accuracy_score(y_test, prediction)*100
lr_p1 = precision_score(y_test, prediction)* 100
lr_r1 = recall_score(y_test, prediction)*100
lr_f11 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=lr1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr1.classes_) 
display.plot()

fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)


fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")

# Display the plot in Streamlit
st.pyplot(fig)



from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(lr1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")


"""### **Decision Tree**"""

dt1=DecisionTreeClassifier(max_depth=15)
dt1.fit(train_c,y_train)
prediction=dt1.predict(test_c)

#EVALUATION
dt_a1 = accuracy_score(y_test, prediction)*100
dt_p1 = precision_score(y_test, prediction)* 100
dt_r1 = recall_score(y_test, prediction)*100
dt_f11 = f1_score(y_test, prediction)*100
print(dt_a1)
print(dt_p1)

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=dt1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr1.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")

# Display the plot in Streamlit
st.pyplot(fig)

from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(dt1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### **Random Forest**"""

rn1=RandomForestClassifier(bootstrap=True,max_depth=15,max_features=5, n_estimators= 100)
rn1.fit(train_c,y_train)
pred_y=rn1.predict(test_c)

#EVALUATION
rn_a1 = accuracy_score(y_test, prediction)*100
rn_p1 = precision_score(y_test, prediction)* 100
rn_r1 = recall_score(y_test, prediction)*100
rn_f11 = f1_score(y_test, prediction)*100
print(rn_a1)
print(rn_p1)

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=dt1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr1.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)


fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")


from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(rn1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

TFIDF_vectorizer  = TfidfVectorizer(stop_words='english')

TFIDF_vectorizer.fit(X_train)
print('\nVocabulary: \n', TFIDF_vectorizer.vocabulary_)

train_tf = TFIDF_vectorizer.fit_transform(X_train)
test_tf = TFIDF_vectorizer.transform(X_test)

"""### Multinomial Naive Bayes model"""

#IMPLEMENTING AND RUNNING MNB MODEL - TFIDF
mnb2 = MultinomialNB()
mnb2.fit(train_tf, y_train)
prediction = mnb2.predict(test_tf)

#EVALUATION
mnb_a2 = accuracy_score(y_test, prediction)*100
mnb_p2 = precision_score(y_test, prediction)* 100
mnb_r2 = recall_score(y_test, prediction)*100

mnb_f12 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=mnb2.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=mnb2.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)


fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")


from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(mnb2)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### Support Vector Machine model"""

#IMPLEMENTING AND RUNNING SVM MODEL - TFIDF 
svm2 = SVC(kernel='linear')
svm2.fit(train_tf, y_train)
prediction = svm2.predict(test_tf)

#EVALUATION
svm_a2 = accuracy_score(y_test, prediction)*100
svm_p2 = precision_score(y_test, prediction)* 100
svm_r2 = recall_score(y_test, prediction)*100
svm_f12 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=svm2.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=svm2.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)


fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")


from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(svm2)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")


"""### Logistic Regression model"""

#IMPLEMENTATION AND RUNNING LR MODEL - TFIDF 
lr2 = LogisticRegression()
lr2.fit(train_tf, y_train)
prediction = lr2.predict(test_tf)

#EVALUATION
lr_a2 = accuracy_score(y_test, prediction)*100
lr_p2 = precision_score(y_test, prediction)* 100
lr_r2 = recall_score(y_test, prediction)*100
lr_f12 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=lr2.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr2.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(lr2)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### **Decision Tree**"""

dt2=DecisionTreeClassifier(max_depth=15)
dt2.fit(train_c,y_train)
prediction=dt2.predict(test_c)

#EVALUATION
dt_a2 = accuracy_score(y_test, prediction)*100
dt_p2 = precision_score(y_test, prediction)* 100
dt_r2 = recall_score(y_test, prediction)*100
dt_f12 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=dt1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr1.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")


from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(dt2)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### **Random Forest**"""

rn2=RandomForestClassifier(bootstrap=True,max_depth=15,max_features=5, n_estimators= 100)
rn2.fit(train_c,y_train)
pred_y=rn2.predict(test_c)

#EVALUATION
rn_a2 = accuracy_score(y_test, prediction)*100
rn_p2 = precision_score(y_test, prediction)* 100
rn_r2 = recall_score(y_test, prediction)*100
rn_f12 = f1_score(y_test, prediction)*100

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, prediction, labels=dt1.classes_)
display = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lr1.classes_) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,prediction)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")

from yellowbrick.classifier import DiscriminationThreshold
visualizer = DiscriminationThreshold(rn1)

visualizer.fit(train_c, y_train)        
visualizer.show(outpath="plot.png")
st.image("plot.png")

"""### COMPARING ACCURACY"""

model_accuracy={'MNB': [round(mnb_a1), round(mnb_a2)],
                'SVM': [round(svm_a1), round(svm_a2)],
                'LR': [round(lr_a1), round(lr_a2)],
                'DT': [round(dt_a1), round(dt_a2)],
                'RN': [round(rn_a1), round(rn_a2)]
               }
ma = pd.DataFrame(model_accuracy, columns = ['MNB','SVM','LR','DT','RN'], index=['Count Vectorizer','Tfidf Vectorizer'])
ma

"""### COMPARING PRECISION"""

model_precision={'MNB': [round(mnb_p1), round(mnb_p2)],
                'SVM': [round(svm_p1), round(svm_p2)],
                'LR': [round(lr_p1), round(lr_p2)],
                 'DT': [round(dt_p1), round(dt_p2)],
                'RN': [round(rn_p1), round(rn_p2)]
               }
mp = pd.DataFrame(model_precision, columns = ['MNB','SVM','LR','DT','RN'], index=['Count Vectorizer','Tfidf Vectorizer'])
mp

"""### COMPARING RECALL"""

model_recall={'MNB': [round(mnb_r1), round(mnb_r2)],
                'SVM': [round(svm_r1), round(svm_r2)],
                'LR': [round(lr_r1), round(lr_r2)],
              'DT': [round(dt_r1), round(dt_r2)],
                'RN': [round(rn_r1), round(rn_r2)]
               }
mr = pd.DataFrame(model_recall, columns = ['MNB','SVM','LR','DT','RN'], index=['Count Vectorizer','Tfidf Vectorizer'])
mr

"""### COMPARING F1 SCORE"""

model_f1={'MNB': [round(mnb_f11), round(mnb_f12)],
                'SVM': [round(svm_f11), round(svm_f12)],
                'LR': [round(lr_f11), round(lr_f12)],
                'DT': [round(dt_r1), round(dt_r2)],
                'RN': [round(rn_r1), round(rn_r2)]
               }
mf1 = pd.DataFrame(model_f1, columns = ['MNB','SVM','LR','DT','RN'], index=['Count Vectorizer','Tfidf Vectorizer'])
mf1

"""# Deep Learning Models

**CNN Model**
"""

def cnn():
    
    input_shape=(train_c.shape[1],)
    sequence = layers.Input(shape=input_shape)
    embeddings = layers.Embedding(len(count_vectorizer.vocabulary_), 128)(sequence)
    
    input= layers.Conv1D(128, 3, activation='relu')(embeddings)
    input= layers.MaxPool1D(3)(input)
    input= layers.Conv1D(64, 5, activation='relu')(input)
    input= layers.MaxPool1D(5)(input)
    input= layers.Conv1D(64, 5, activation='relu')(input)
    input= layers.GlobalMaxPool1D()(input)
    input= layers.Flatten()(input)
    input= layers.Dense(128, activation='relu')(input)
    
    pred = layers.Dense(1, activation='sigmoid')(input)
    
    model = models.Model(inputs=sequence, outputs=pred)
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    return model
    
model_cnn = cnn()

history = model_cnn.fit(
    train_c.toarray(), 
    y_train, 
    batch_size=32 ,
    epochs=10,verbose=1)

### Test Accuracy
model_cnn.evaluate(test_c.toarray(),y_test)

predictions_cnn = model_cnn.predict(test_c.toarray())
predictions_cnn = [True if x >= 0.5 else False for x in predictions_cnn]

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, predictions_cnn)
display = ConfusionMatrixDisplay(confusion_matrix=cm) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,predictions_cnn)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.figure(figsize=(8,6))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")

fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")

"""**ANN** **Model**"""

def ann():
  model = Sequential()
  model.add(Dense(units = 128 , activation = 'relu' , input_dim = train_c.shape[1]))
  model.add(Dense(units = 64 , activation = 'relu'))
  model.add(Dense(units = 32 , activation = 'relu'))
  model.add(Dense(units = 16 , activation = 'relu'))
  model.add(Dense(units = 1 , activation = 'sigmoid'))

  model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])
  return model

model_ann = ann()
model_ann.fit(train_c.toarray(),y_train , epochs = 5,batch_size=32)

### Test Accuracy
model_ann.evaluate(test_c.toarray(),y_test)

predictions_ann = model_ann.predict(test_c.toarray())
predictions_ann = [True if x >= 0.5 else False for x in predictions_ann]

#CONFUSION MATRIX
cm =  confusion_matrix(y_test, predictions_ann)
display = ConfusionMatrixDisplay(confusion_matrix=cm) 
display.plot()
fig, ax = plt.subplots()
display.plot(ax=ax)
ax.set_title("Confusion Matrix")

# Display the plot in Streamlit
st.pyplot(fig)

fpr, tpr, thresholds_roc = roc_curve(y_test,predictions_ann)
roc_auc = auc(fpr,tpr)
print("The area under curve {}".format(roc_auc))
plt.figure(figsize=(8,6))
plt.plot(fpr,tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
fig, ax = plt.subplots()
ax.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
ax.plot([0, 1], [0, 1], 'k--', lw=2)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curve')
ax.legend(loc="lower right")