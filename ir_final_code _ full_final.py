# -*- coding: utf-8 -*-
"""IR_Final_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12-Ou-f1GgBUPaJ2V4CA_NjCWyDS04pVO
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""## Importing Packages"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import random
import os
import gc
from PIL import Image
from textwrap import wrap
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import spacy
from tensorflow.python.keras import models, layers, optimizers
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.convolutional import Conv1D
from keras.layers import Embedding
from keras.layers import GlobalMaxPooling1D
from keras.layers.core import Activation, Dropout, Dense
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
import bz2
import re
import streamlit as st
# %matplotlib inline

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



# Any results you write to the current directory are saved as output.

train = 'train.ft.txt.bz2'
test = 'test.ft.txt.bz2'
users = 'Reviews.csv'

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import transformers
from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification

tf.random.set_seed(18)
np.random.seed(18)

# traindata = pd.read_csv(train,delimiter='\t', error_bad_lines=False, encoding='utf-8')
# testdata = pd.read_csv(test,delimiter='\t', error_bad_lines=False, encoding='utf-8')

# traindata.shape

d=pd.read_csv(users)



def get_labels_and_texts(file):
    labels = []
    texts = []

    for line in bz2.BZ2File(file):
        x = line.decode("utf-8")
        labels.append(int(x[9]) - 1)
        texts.append(x[10:].strip())
    #labels = labels[:int(len(labels)*0.03)]
    #texts = texts[:int(len(texts)*0.03)]
    return np.array(labels), texts
train_labels, train_texts = get_labels_and_texts(train)
test_labels, test_texts = get_labels_and_texts(test)

df=pd.DataFrame(columns=['label','text','user_id','username'])
df['label'] = train_labels
df['text'] = train_texts
df['user_id']=d['UserId']
df['username']=d['ProfileName']
df['product_id']=d['ProductId']

df= df.sample(n=108000)

# print(df)

# df['text'][3]
print(df)

from sklearn.model_selection import train_test_split
train_labels, test_labels, train_texts , test_texts = train_test_split(df['label'],df['text'] ,
                                   random_state=104, 
                                   test_size=0.25, 
                                   shuffle=True)

NON_ALPHANUM = re.compile(r'[\W]')
NON_ASCII = re.compile(r'[^a-z0-1\s]')
def normalize_texts(texts):
    normalized_texts = []
    for text in texts:
        lower = text.lower()
        no_punctuation = NON_ALPHANUM.sub(r' ', lower)
        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)
        normalized_texts.append(no_non_ascii)
    return normalized_texts
        
train_texts = normalize_texts(train_texts)
test_texts = normalize_texts(test_texts)

train_texts_Bert = train_texts
train_labels_Bert = train_labels
test_texts_Bert = test_texts
test_labels_Bert = test_labels
train_reviews = train_texts

# """#### Train/Validation Split
# We split the dataset, keeping 20% of the training set for validation.
# """

train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, random_state=57643892, test_size=0.2)

MAX_FEATURES = 12000
#Tokenize texts
tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(train_texts)
train_texts = tokenizer.texts_to_sequences(train_texts)
val_texts = tokenizer.texts_to_sequences(val_texts)
test_texts = tokenizer.texts_to_sequences(test_texts)

print(train_texts[0])

MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)
#Add padding
train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)
val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)
test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)

# train_texts[0]

# """### Long Short-Term Memory (LSTM)"""

def build_lstm_model():
    sequences = layers.Input(shape=(MAX_LENGTH,))
    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)
    x = layers.LSTM(32, return_sequences=True)(embedded)
    x = layers.GlobalMaxPool1D()(x)
    x = layers.Dense(16, activation='relu')(x)
    x = layers.Dropout(0.2)(x)
    predictions = layers.Dense(1, activation='sigmoid')(x)
    model = models.Model(inputs=sequences, outputs=predictions)
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model
    
lstm_model = build_lstm_model()

print(lstm_model.summary())

lstm_model.fit(
    train_texts, 
    train_labels, 
    batch_size=128,
    epochs=2,
    validation_data=(val_texts, val_labels), )



preds = lstm_model.predict(test_texts)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))
print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))
print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))
st.write('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))
st.write('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))
st.write('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))

query = st.text_input("Enter query here:")
# query = "This is a positive statement."

# # Tokenize and pad the query
query_seq = tokenizer.texts_to_sequences([query])
query_seq_padded = pad_sequences(query_seq, maxlen=MAX_LENGTH)

# Make a prediction using the LSTM model
prediction = lstm_model.predict(query_seq_padded)

# Output the prediction
if prediction > 0.5:
    print("The query is classified as positive.")
    st.write("The query is classified as positive.")
else:
    print("The query is classified as negative.")
    st.write("The query is classified as negative.")



"""### BERT

For BERT we do same split on train set (20% for validation)
"""

train_texts_Bert, val_texts_Bert, train_labels_Bert, val_labels_Bert = train_test_split(
    train_texts_Bert, train_labels_Bert, random_state=57643892, test_size=0.2)

#Assign tokenizer object to the tokenizer class
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts_Bert,
                            truncation=True,
                            padding=True,
                            max_length=256)
val_encodings = tokenizer(val_texts_Bert,
                          truncation=True,
                          padding=True,
                          max_length=256)
train_dataset = tf.data.Dataset.from_tensor_slices((
                            dict(train_encodings),
                            train_labels_Bert
                            ))
val_dataset = tf.data.Dataset.from_tensor_slices((
                            dict(val_encodings),
                            val_labels_Bert
                            ))
test_encodings = tokenizer(test_texts_Bert,
                          truncation=True,
                          padding=True,
                          max_length=256)
test_dataset = tf.data.Dataset.from_tensor_slices((
                            dict(test_encodings),
                            test_labels_Bert
                            ))

bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)

print(bert_model.summary())

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)
bert_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=["binary_accuracy"])
bert_model.fit(train_dataset.shuffle(100).batch(16),
          epochs=2,
          batch_size=16,
          validation_data=val_dataset.shuffle(100).batch(16))

results_bert = bert_model.evaluate(test_dataset.batch(8))

bert_model.save_weights('bert.h5')

query = st.text_input("Enter query here:")
# query = "the product is expensive for this price, must buy this item "
encoding = tokenizer.encode_plus(query, max_length=MAX_LENGTH, truncation=True, padding='max_length', return_tensors='tf')
outputs = bert_model(encoding)
rel_val = outputs.logits.numpy()[0]
predicted_label = outputs.logits.numpy()[0] > 0.5  # assuming threshold of 0.5
print(predicted_label[0],rel_val)
st.write(predicted_label[0],rel_val)